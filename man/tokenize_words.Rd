% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tokenize_words.R
\name{tokenize_words}
\alias{tokenize_words}
\title{Tokenizar texto en palabras}
\usage{
tokenize_words(
  text,
  lowercase = TRUE,
  keep_accents = TRUE,
  strip_punct = TRUE,
  keep_hyphens = TRUE,
  remove_numbers = FALSE,
  strip_symbols = FALSE,
  flatten = FALSE
)
}
\arguments{
\item{text}{Vector de caracteres que contiene el texto a tokenizar.}

\item{lowercase}{Lógico; si es \code{TRUE}, convierte todo el texto a minúsculas antes de tokenizar.}

\item{keep_accents}{Lógico; si es \code{FALSE}, se eliminan las tildes de los caracteres.}

\item{strip_punct}{Lógico; si es \code{TRUE}, elimina la puntuación antes de tokenizar.}

\item{keep_hyphens}{Lógico; si es \code{TRUE}, conserva los guiones dentro de las palabras (por ejemplo, \emph{teórico-práctico}).}

\item{remove_numbers}{Lógico; si es \code{TRUE}, elimina los tokens que son puramente numéricos.}
}
\value{
Una lista donde cada elemento corresponde a los tokens obtenidos de cada elemento de \code{text}.
}
\description{
Divide un vector de texto en tokens individuales de palabras, utilizando
reglas adaptadas para texto en español. La función puede, opcionalmente,
normalizar el uso de mayúsculas, conservar o eliminar tildes, y decidir cómo
tratar guiones y números.
}
\examples{
tokenize_words("Este es un texto de prueba, con tildes y números: 123.")
tokenize_words(c("Primera frase.", "Segunda frase con más palabras."))
}
