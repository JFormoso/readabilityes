test_that("tokenize_words existe y es funci√≥n", {
  expect_true(exists("tokenize_words"))
  expect_true(is.function(tokenize_words))
})

test_that("entrada inv√°lida lanza error; vector vac√≠o devuelve lista vac√≠a", {
  expect_error(tokenize_words(NA), regexp = "debe ser un vector de caracteres")
  out <- tokenize_words(character())
  expect_type(out, "list")
  expect_length(out, 0)
})

test_that("por defecto: min√∫sculas, tildes preservadas, puntuaci√≥n fuera, guiones dentro", {
  txt <- "¬°Hola, mundo! A√±o 2025: ping√ºino, ma√Øz y NI√ëA."
  out <- tokenize_words(txt)            # lista de longitud 1
  expect_type(out, "list")
  expect_length(out, 1)
  toks <- out[[1]]
  expect_true(length(toks) >= 6)
  # lowercase = TRUE
  expect_true(all(toks == tolower(toks)))
  # strip_punct = TRUE, keep_hyphens = TRUE
  expect_true(all(!grepl("[[:punct:]]", toks)))
  # keep_accents = TRUE
  expect_true(all(c("hola","mundo","a√±o","ping√ºino","ma√Øz","ni√±a") %in% toks))
})

test_that("lowercase = FALSE conserva may√∫sculas", {
  txt <- "Hola NI√ëA Caf√©"
  toks <- tokenize_words(txt, lowercase = FALSE)[[1]]
  expect_true(any(grepl("[A-Z√Å√â√ç√ì√ö√ë]", toks)))
})

test_that("keep_accents = FALSE elimina tildes/di√©resis", {
  txt <- "canci√≥n ping√ºino ma√Øz NI√ëA"
  kept     <- tokenize_words(txt, keep_accents = TRUE)[[1]]
  stripped <- tokenize_words(txt, keep_accents = FALSE)[[1]]
  expect_true(all(c("canci√≥n","ping√ºino","ma√Øz","ni√±a") %in% kept))
  expect_true(all(c("cancion","pinguino","maiz","nina") %in% stripped))
})

test_that("strip_punct y keep_hyphens: control de puntuaci√≥n y guiones", {
  txt <- "auto-piloto, co-autor; siglo-XX. ¬°Listo?"
  # default (strip_punct=TRUE, keep_hyphens=TRUE) conserva '-'
  keep <- tokenize_words(txt)[[1]]
  expect_true(any(grepl("-", keep)))
  expect_false(any(grepl("[[:punct:]&&[^-]]", keep)))

  # sin conservar guiones: se separa en tokens sin '-'
  split <- tokenize_words(txt, keep_hyphens = FALSE)[[1]]
  expect_false(any(grepl("-", split)))
  expect_true(all(c("auto","piloto","co","autor","siglo","xx","listo") %in% split))

  # sin quitar puntuaci√≥n: quedan signos pegados
  no_strip <- tokenize_words("hola!", strip_punct = FALSE)[[1]]
  expect_true("hola!" %in% no_strip)
})

test_that("remove_numbers = TRUE descarta tokens puramente num√©ricos", {
  txt <- "r2d2 a√±o2025 123 45rpm"
  kept <- tokenize_words(txt, remove_numbers = FALSE)[[1]]
  drop <- tokenize_words(txt, remove_numbers = TRUE)[[1]]

  expect_true("123" %in% kept)
  expect_false("123" %in% drop)

  # tokens alfanum√©ricos se conservan en ambos casos
  expect_true(all(c("r2d2","a√±o2025","45rpm") %in% kept))
  expect_true(all(c("r2d2","a√±o2025","45rpm") %in% drop))
})

test_that("strip_symbols = TRUE elimina s√≠mbolos/emojis", {
  txt <- "caf√©‚òï y coraz√≥nüíñ *precio* 50‚Ç¨"
  kept   <- tokenize_words(txt, strip_symbols = FALSE)[[1]]
  stripped <- tokenize_words(txt, strip_symbols = TRUE)[[1]]

  # Sin quitar s√≠mbolos, pueden quedar pegados
  expect_true(any(grepl("‚òï|üíñ|‚Ç¨", paste(kept, collapse = " "))))
  # Quitando s√≠mbolos, no deber√≠an aparecer
  expect_false(any(grepl("‚òï|üíñ|‚Ç¨", paste(stripped, collapse = " "))))

  # El contenido l√©xico debe seguir presente
  expect_true(all(c("caf√©","y","coraz√≥n","precio","50") %in% stripped))
})

test_that("flatten = TRUE retorna vector cuando length(text) == 1", {
  txt <- "Hola mundo cruel"
  out <- tokenize_words(txt, flatten = TRUE)
  expect_type(out, "character")
  expect_gt(length(out), 0)

  # Si hay m√°s de un elemento en text, flatten sigue devolviendo lista (seg√∫n tu implementaci√≥n)
  out2 <- tokenize_words(c("uno dos", "tres"), flatten = TRUE)
  expect_type(out2, "list")
  expect_length(out2, 2)
})

test_that("combinaciones t√≠picas: min_len conceptual (no existe), pero se puede emular", {
  # No ten√©s min_len; emulemos filtrando nchar >= 3 post-tokenizaci√≥n
  txt <- "A la luz del sol"
  toks <- tokenize_words(txt)[[1]]
  expect_true(all(toks == tolower(toks)))
  expect_true(all(c("la","luz","del","sol") %in% toks))
  toks3 <- toks[nchar(toks) >= 3]
  expect_true(all(nchar(toks3) >= 3))
})

test_that("consistencia b√°sica con clean_word/tokenize_clean si existen", {
  skip_if_not(exists("clean_word") && exists("tokenize_clean"))

  words <- c("Hola!", "  NI√ëA  ", "ma√Øz", "ping√ºino", "r2d2")
  for (w in words) {
    # Alineamos pol√≠ticas razonables con clean_word:
    toks <- tokenize_words(
      w,
      strip_symbols = TRUE,  # suele limpiar s√≠mbolos como hace clean_word
      strip_punct   = TRUE,
      keep_hyphens  = TRUE,
      flatten       = TRUE
    )

    cw <- clean_word(w)
    # Normalizamos cada token con clean_word y filtramos vac√≠os/NA
    tk_norm <- vapply(toks, clean_word, character(1))
    tk_norm <- tolower(tk_norm[!is.na(tk_norm) & nzchar(tk_norm)])

    if (!is.na(cw) && nzchar(cw)) {
      # La versi√≥n limpia de la palabra debe aparecer entre los tokens limpiados
      expect_true(tolower(cw) %in% tk_norm)
    } else {
      # Si clean_word deja vac√≠o/NA, entonces ning√∫n token limpio deber√≠a quedar
      expect_length(tk_norm, 0L)
    }

    # Verificaci√≥n adicional: si NO removemos puntuaci√≥n, el signo se conserva
    if (w == "Hola!") {
      tk_nostrip <- tokenize_words(w, strip_punct = FALSE, flatten = TRUE)
      expect_true("hola!" %in% tolower(tk_nostrip))
    }
  }
})


